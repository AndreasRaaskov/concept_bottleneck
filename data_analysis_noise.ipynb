{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.notebook import display_scrollable_dataframe,plot_sailency\n",
    "from data_loaders import CUB_extnded_dataset\n",
    "from models import get_inception_transform\n",
    "from IPython.display import display\n",
    "\n",
    "from sailency import get_saliency_maps,saliency_score_part\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analysis \n",
    "On the use of Majority voting on the concetps. First the data is loaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings for the experiment your running\n",
    "\n",
    "# Settings for loading the dataset, make sure its the same as the one used to train the model. \n",
    "model_folder_MV = r\"models\\Sequential_Basemodel3\"\n",
    "model_folder_NoMV = r\"models\\Sequential_Basemodel2\"\n",
    "\n",
    "\n",
    "use_majority_voting_MV = True\n",
    "use_majority_voting_NoMV = False\n",
    "\n",
    "# You can find the settings in the .hydra folders config.yaml \n",
    "data_config_MV = {'CUB_dir':r'data/CUB_200_2011',\n",
    "                'split_file':r'data/train_test_val.pkl',\n",
    "                'use_majority_voting':use_majority_voting_MV,\n",
    "                'min_class_count':10,\n",
    "                'return_visibility':True}\n",
    "\n",
    "\n",
    "# You can find the settings in the .hydra folders config.yaml \n",
    "data_config_NoMV = {'CUB_dir':r'data/CUB_200_2011',\n",
    "                'split_file':r'data/train_test_val.pkl',\n",
    "                'use_majority_voting':use_majority_voting_NoMV,\n",
    "                'min_class_count':10,\n",
    "                'return_visibility':True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112 312\n"
     ]
    }
   ],
   "source": [
    "data_set = 'val' # majority voting is only done  \n",
    "\n",
    "#Define data set, the human transformer data_set is used to get the original images instead of the normalized ones\n",
    "transformer = get_inception_transform(mode=data_set,methode=\"center\")\n",
    "human_tansform = torchvision.transforms.Compose([torchvision.transforms.CenterCrop(299),torchvision.transforms.ToTensor()])\n",
    "\n",
    "#Get dataset\n",
    "data_MV_val = CUB_extnded_dataset(mode=data_set,config_dict=data_config_MV,transform=transformer)\n",
    "data_NoMV_val = CUB_extnded_dataset(mode=data_set,config_dict=data_config_NoMV,transform=transformer)\n",
    "\n",
    "#Get a dataset that return original images instead normalized ones\n",
    "data_human_MV_val = CUB_extnded_dataset(mode=data_set,config_dict=data_config_MV,transform=human_tansform)\n",
    "concept_names_MV_val = data_MV_val.consept_labels_names\n",
    "class_names_MV_val = data_MV_val.class_labels_names\n",
    "n_classes_MV_val = data_MV_val.n_classes\n",
    "n_concepts_MV_val = data_MV_val.n_concepts\n",
    "\n",
    "data_human_NoMV_val = CUB_extnded_dataset(mode=data_set,config_dict=data_config_NoMV,transform=human_tansform)\n",
    "concept_names_NoMV_val = data_NoMV_val.consept_labels_names\n",
    "class_names_NoMV_val = data_NoMV_val.class_labels_names\n",
    "n_classes_NoMV_val = data_NoMV_val.n_classes\n",
    "n_concepts_NoMV_val = data_NoMV_val.n_concepts\n",
    "\n",
    "\n",
    "print(n_concepts_MV_val,n_concepts_NoMV_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 299, 299])\n",
      "torch.Size([112])\n",
      "torch.Size([200])\n",
      "112\n",
      "torch.Size([3, 299, 299])\n",
      "torch.Size([2, 312])\n",
      "torch.Size([200])\n",
      "312\n"
     ]
    }
   ],
   "source": [
    "print(data_MV_val[0][0].shape) # image\n",
    "print(data_MV_val[0][1].shape) # concepts and visibility\n",
    "print(data_MV_val[0][2].shape) # classes\n",
    "print(len(data_MV_val[0][3])) # concept coordinates\n",
    "\n",
    "print(data_NoMV_val[0][0].shape) # image\n",
    "print(data_NoMV_val[0][1].shape) # concepts and visibility\n",
    "print(data_NoMV_val[0][2].shape) # classes\n",
    "print(len(data_NoMV_val[0][3])) # concept coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112 312\n"
     ]
    }
   ],
   "source": [
    "data_set = 'train' # majority voting is only done  \n",
    "\n",
    "#Define data set, the human transformer data_set is used to get the original images instead of the normalized ones\n",
    "transformer = get_inception_transform(mode=data_set,methode=\"center\")\n",
    "human_tansform = torchvision.transforms.Compose([torchvision.transforms.CenterCrop(299),torchvision.transforms.ToTensor()])\n",
    "\n",
    "#Get dataset\n",
    "data_MV_train = CUB_extnded_dataset(mode=data_set,config_dict=data_config_MV,transform=transformer)\n",
    "data_NoMV_train = CUB_extnded_dataset(mode=data_set,config_dict=data_config_NoMV,transform=transformer)\n",
    "\n",
    "#Get a dataset that return original images instead normalized ones\n",
    "data_human_MV_train = CUB_extnded_dataset(mode=data_set,config_dict=data_config_MV,transform=human_tansform)\n",
    "concept_names_MV_train = data_MV_train.consept_labels_names\n",
    "class_names_MV_train = data_MV_train.class_labels_names\n",
    "n_classes_MV_train = data_MV_train.n_classes\n",
    "n_concepts_MV_train = data_MV_train.n_concepts\n",
    "\n",
    "data_human_NoMV_train = CUB_extnded_dataset(mode=data_set,config_dict=data_config_NoMV,transform=human_tansform)\n",
    "concept_names_NoMV_train = data_NoMV_train.consept_labels_names\n",
    "class_names_NoMV_train = data_NoMV_train.class_labels_names\n",
    "n_classes_NoMV_train = data_NoMV_train.n_classes\n",
    "n_concepts_NoMV_train = data_NoMV_train.n_concepts\n",
    "\n",
    "\n",
    "print(n_concepts_MV_train,n_concepts_NoMV_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 299, 299])\n",
      "torch.Size([112])\n",
      "torch.Size([200])\n",
      "112\n",
      "torch.Size([3, 299, 299])\n",
      "torch.Size([2, 312])\n",
      "torch.Size([200])\n",
      "312\n"
     ]
    }
   ],
   "source": [
    "print(data_MV_train[0][0].shape) # image\n",
    "print(data_MV_train[0][1].shape) # concepts and visibility\n",
    "print(data_MV_train[0][2].shape) # classes\n",
    "print(len(data_MV_train[0][3])) # concept coordinates, but only the mask needs to be applied\n",
    "\n",
    "print(data_NoMV_train[0][0].shape) # image\n",
    "print(data_NoMV_train[0][1].shape) # concepts and visibility\n",
    "print(data_NoMV_train[0][2].shape) # classes\n",
    "print(len(data_NoMV_train[0][3])) # concept coordinates, but only the mask needs to be applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312 312\n"
     ]
    }
   ],
   "source": [
    "data_set = 'val' # majority voting is only done  \n",
    "\n",
    "#Define data set, the human transformer data_set is used to get the original images instead of the normalized ones\n",
    "transformer = get_inception_transform(mode=data_set,methode=\"center\")\n",
    "human_tansform = torchvision.transforms.Compose([torchvision.transforms.CenterCrop(299),torchvision.transforms.ToTensor()])\n",
    "\n",
    "#Get dataset\n",
    "data_MV_val = CUB_extnded_dataset(mode=data_set,config_dict=data_config_MV,transform=transformer, reduce = False)\n",
    "data_NoMV_val = CUB_extnded_dataset(mode=data_set,config_dict=data_config_NoMV,transform=transformer, reduce = False)\n",
    "\n",
    "#Get a dataset that return original images instead normalized ones\n",
    "data_human_MV_val = CUB_extnded_dataset(mode=data_set,config_dict=data_config_MV,transform=human_tansform, reduce = False)\n",
    "concept_names_MV_val = data_MV_val.consept_labels_names\n",
    "class_names_MV_val = data_MV_val.class_labels_names\n",
    "n_classes_MV_val = data_MV_val.n_classes\n",
    "n_concepts_MV_val = data_MV_val.n_concepts\n",
    "\n",
    "data_human_NoMV_val = CUB_extnded_dataset(mode=data_set,config_dict=data_config_NoMV,transform=human_tansform, reduce = False)\n",
    "concept_names_NoMV_val = data_NoMV_val.consept_labels_names\n",
    "class_names_NoMV_val = data_NoMV_val.class_labels_names\n",
    "n_classes_NoMV_val = data_NoMV_val.n_classes\n",
    "n_concepts_NoMV_val = data_NoMV_val.n_concepts\n",
    "\n",
    "\n",
    "print(n_concepts_MV_val,n_concepts_NoMV_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 299, 299])\n",
      "torch.Size([312])\n",
      "torch.Size([200])\n",
      "112\n",
      "torch.Size([3, 299, 299])\n",
      "torch.Size([2, 312])\n",
      "torch.Size([200])\n",
      "312\n"
     ]
    }
   ],
   "source": [
    "print(data_MV_val[0][0].shape) # image\n",
    "print(data_MV_val[0][1].shape) # concepts\n",
    "print(data_MV_val[0][2].shape) # classes\n",
    "print(len(data_MV_val[0][3])) # concept coordinates, but only the mask needs to be applied\n",
    "\n",
    "print(data_NoMV_val[0][0].shape) # image\n",
    "print(data_NoMV_val[0][1].shape) # concepts\n",
    "print(data_NoMV_val[0][2].shape) # classes\n",
    "print(len(data_NoMV_val[0][3])) # concept coordinates, but only the mask needs to be applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312 312\n"
     ]
    }
   ],
   "source": [
    "data_set = 'train' # majority voting is only done  \n",
    "\n",
    "#Define data set, the human transformer data_set is used to get the original images instead of the normalized ones\n",
    "transformer = get_inception_transform(mode=data_set,methode=\"center\")\n",
    "human_tansform = torchvision.transforms.Compose([torchvision.transforms.CenterCrop(299),torchvision.transforms.ToTensor()])\n",
    "\n",
    "#Get dataset\n",
    "data_MV_train = CUB_extnded_dataset(mode=data_set,config_dict=data_config_MV,transform=transformer, reduce = False)\n",
    "data_NoMV_train = CUB_extnded_dataset(mode=data_set,config_dict=data_config_NoMV,transform=transformer, reduce = False)\n",
    "\n",
    "#Get a dataset that return original images instead normalized ones\n",
    "data_human_MV_train = CUB_extnded_dataset(mode=data_set,config_dict=data_config_MV,transform=human_tansform, reduce = False)\n",
    "concept_names_MV_train = data_MV_train.consept_labels_names\n",
    "class_names_MV_train = data_MV_train.class_labels_names\n",
    "n_classes_MV_train = data_MV_train.n_classes\n",
    "n_concepts_MV_train = data_MV_train.n_concepts\n",
    "\n",
    "data_human_NoMV_train = CUB_extnded_dataset(mode=data_set,config_dict=data_config_NoMV,transform=human_tansform, reduce = False)\n",
    "concept_names_NoMV_train = data_NoMV_train.consept_labels_names\n",
    "class_names_NoMV_train = data_NoMV_train.class_labels_names\n",
    "n_classes_NoMV_train = data_NoMV_train.n_classes\n",
    "n_concepts_NoMV_train = data_NoMV_train.n_concepts\n",
    "\n",
    "\n",
    "print(n_concepts_MV_train,n_concepts_NoMV_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 299, 299])\n",
      "torch.Size([312])\n",
      "torch.Size([200])\n",
      "112\n",
      "torch.Size([3, 299, 299])\n",
      "torch.Size([2, 312])\n",
      "torch.Size([200])\n",
      "312\n"
     ]
    }
   ],
   "source": [
    "print(data_MV_train[0][0].shape) # image\n",
    "print(data_MV_train[0][1].shape) # concepts\n",
    "print(data_MV_train[0][2].shape) # classes\n",
    "print(len(data_MV_train[0][3])) # concept coordinates, but only the mask needs to be applied\n",
    "\n",
    "print(data_NoMV_train[0][0].shape) # image\n",
    "print(data_NoMV_train[0][1].shape) # concepts\n",
    "print(data_NoMV_train[0][2].shape) # classes\n",
    "print(len(data_NoMV_train[0][3])) # concept coordinates, but only the mask needs to be applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1198/1198 [00:26<00:00, 45.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# compare all the concepts in data_MV_train to data_noMV_train\n",
    "diff = 0\n",
    "for item_MV, item_NoMV in zip(tqdm(data_MV_val), data_NoMV_val):\n",
    "\n",
    "    diff_i = torch.sum(item_MV[1] != item_NoMV[1][0])\n",
    "    diff += diff_i\n",
    "\n",
    "print(diff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4796/4796 [01:17<00:00, 61.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(149144)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# compare all the concepts in data_MV_train to data_noMV_train\n",
    "diff = 0\n",
    "for item_MV, item_NoMV in zip(tqdm(data_MV_train), data_NoMV_train):\n",
    "\n",
    "    diff_i = torch.sum(item_MV[1] != item_NoMV[1][0])\n",
    "    diff += diff_i\n",
    "\n",
    "print(diff)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1496352\n",
      "tensor(0.0997)\n"
     ]
    }
   ],
   "source": [
    "print(len(data_MV_train)*312)\n",
    "print(diff/(len(data_MV_train)*312))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Difference in all concepts\n",
    "When changing the concepts with majority voting, is there some classes that is more affected by this change? Look at some examples of these classes. Are there some tendencies in the classes that are more affected by the change maybe the change is due to different genders in the class?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What classes has the most non visible concepts in MV?\n",
    "When doing MV you can introduce concepts into a sample that is not visible. What classes is most affected by this?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "con_bot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

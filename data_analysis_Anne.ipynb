{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from utils.notebook import display_scrollable_dataframe,plot_sailency\n",
    "from data_loaders import CUB_extnded_dataset\n",
    "from models import get_inception_transform\n",
    "from IPython.display import display\n",
    "\n",
    "from sailency import get_saliency_maps,saliency_score_part\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analysis \n",
    "On the use of Majority voting on the concetps. First the data is loaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings for the experiment your running\n",
    "\n",
    "# Settings for loading the dataset, make sure its the same as the one used to train the model. \n",
    "model_folder_MV = r\"models\\Sequential_Basemodel3\"\n",
    "model_folder_NoMV = r\"models\\Sequential_Basemodel2\"\n",
    "\n",
    "\n",
    "use_majority_voting_MV = True\n",
    "use_majority_voting_NoMV = False\n",
    "\n",
    "# You can find the settings in the .hydra folders config.yaml \n",
    "data_config_MV = {'CUB_dir':r'data/CUB_200_2011',\n",
    "                'split_file':r'data/train_test_val.pkl',\n",
    "                'use_majority_voting':use_majority_voting_MV,\n",
    "                'min_class_count':10,\n",
    "                'return_visibility':True}\n",
    "\n",
    "\n",
    "# You can find the settings in the .hydra folders config.yaml \n",
    "data_config_NoMV = {'CUB_dir':r'data/CUB_200_2011',\n",
    "                'split_file':r'data/train_test_val.pkl',\n",
    "                'use_majority_voting':use_majority_voting_NoMV,\n",
    "                'min_class_count':10,\n",
    "                'return_visibility':True}\n",
    "\n",
    "#Load models make sure the model is trained with the same settings as the data loader\n",
    "\n",
    "X_to_C_path = os.path.join(model_folder_MV,\"best_XtoC_model.pth\")\n",
    "C_to_Y_path = os.path.join(model_folder_MV,\"best_CtoY_model.pth\")\n",
    "\n",
    "ModelXtoC_MV = torch.load(X_to_C_path,map_location=torch.device('cpu'))\n",
    "ModelCtoY_MV = torch.load(C_to_Y_path,map_location=torch.device('cpu'))\n",
    "\n",
    "X_to_C_path = os.path.join(model_folder_NoMV,\"best_XtoC_model.pth\")\n",
    "C_to_Y_path = os.path.join(model_folder_NoMV,\"best_CtoY_model.pth\")\n",
    "\n",
    "ModelXtoC_NoMV = torch.load(X_to_C_path,map_location=torch.device('cpu'))\n",
    "ModelCtoY_NoMV = torch.load(C_to_Y_path,map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112 312\n"
     ]
    }
   ],
   "source": [
    "data_set = 'val' # majority voting is only done  \n",
    "\n",
    "#Define data set, the human transformer data_set is used to get the original images instead of the normalized ones\n",
    "transformer = get_inception_transform(mode=data_set,methode=\"center\")\n",
    "human_tansform = torchvision.transforms.Compose([torchvision.transforms.CenterCrop(299),torchvision.transforms.ToTensor()])\n",
    "\n",
    "#Get dataset\n",
    "data_MV_val = CUB_extnded_dataset(mode=data_set,config_dict=data_config_MV,transform=transformer)\n",
    "data_NoMV_val = CUB_extnded_dataset(mode=data_set,config_dict=data_config_NoMV,transform=transformer)\n",
    "\n",
    "#Get a dataset that return original images instead normalized ones\n",
    "data_human_MV_val = CUB_extnded_dataset(mode=data_set,config_dict=data_config_MV,transform=human_tansform)\n",
    "concept_names_MV_val = data_MV_val.consept_labels_names\n",
    "class_names_MV_val = data_MV_val.class_labels_names\n",
    "n_classes_MV_val = data_MV_val.n_classes\n",
    "n_concepts_MV_val = data_MV_val.n_concepts\n",
    "\n",
    "data_human_NoMV_val = CUB_extnded_dataset(mode=data_set,config_dict=data_config_NoMV,transform=human_tansform)\n",
    "concept_names_NoMV_val = data_NoMV_val.consept_labels_names\n",
    "class_names_NoMV_val = data_NoMV_val.class_labels_names\n",
    "n_classes_NoMV_val = data_NoMV_val.n_classes\n",
    "n_concepts_NoMV_val = data_NoMV_val.n_concepts\n",
    "\n",
    "\n",
    "print(n_concepts_MV_val,n_concepts_NoMV_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 299, 299])\n",
      "torch.Size([112])\n",
      "torch.Size([200])\n",
      "112\n",
      "torch.Size([3, 299, 299])\n",
      "torch.Size([2, 312])\n",
      "torch.Size([200])\n",
      "312\n"
     ]
    }
   ],
   "source": [
    "print(data_MV_val[0][0].shape) # image\n",
    "print(data_MV_val[0][1].shape) # concepts and visibility\n",
    "print(data_MV_val[0][2].shape) # classes\n",
    "print(len(data_MV_val[0][3])) # concept coordinates\n",
    "\n",
    "print(data_NoMV_val[0][0].shape) # image\n",
    "print(data_NoMV_val[0][1].shape) # concepts and visibility\n",
    "print(data_NoMV_val[0][2].shape) # classes\n",
    "print(len(data_NoMV_val[0][3])) # concept coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112 312\n"
     ]
    }
   ],
   "source": [
    "data_set = 'train' # majority voting is only done  \n",
    "\n",
    "#Define data set, the human transformer data_set is used to get the original images instead of the normalized ones\n",
    "transformer = get_inception_transform(mode=data_set,methode=\"center\")\n",
    "human_tansform = torchvision.transforms.Compose([torchvision.transforms.CenterCrop(299),torchvision.transforms.ToTensor()])\n",
    "\n",
    "#Get dataset\n",
    "data_MV_train = CUB_extnded_dataset(mode=data_set,config_dict=data_config_MV,transform=transformer)\n",
    "data_NoMV_train = CUB_extnded_dataset(mode=data_set,config_dict=data_config_NoMV,transform=transformer)\n",
    "\n",
    "#Get a dataset that return original images instead normalized ones\n",
    "data_human_MV_train = CUB_extnded_dataset(mode=data_set,config_dict=data_config_MV,transform=human_tansform)\n",
    "concept_names_MV_train = data_MV_train.consept_labels_names\n",
    "class_names_MV_train = data_MV_train.class_labels_names\n",
    "n_classes_MV_train = data_MV_train.n_classes\n",
    "n_concepts_MV_train = data_MV_train.n_concepts\n",
    "\n",
    "data_human_NoMV_train = CUB_extnded_dataset(mode=data_set,config_dict=data_config_NoMV,transform=human_tansform)\n",
    "concept_names_NoMV_train = data_NoMV_train.consept_labels_names\n",
    "class_names_NoMV_train = data_NoMV_train.class_labels_names\n",
    "n_classes_NoMV_train = data_NoMV_train.n_classes\n",
    "n_concepts_NoMV_train = data_NoMV_train.n_concepts\n",
    "\n",
    "\n",
    "print(n_concepts_MV_train,n_concepts_NoMV_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 299, 299])\n",
      "torch.Size([112])\n",
      "torch.Size([200])\n",
      "112\n",
      "torch.Size([3, 299, 299])\n",
      "torch.Size([2, 312])\n",
      "torch.Size([200])\n",
      "312\n"
     ]
    }
   ],
   "source": [
    "print(data_MV_train[0][0].shape) # image\n",
    "print(data_MV_train[0][1].shape) # concepts and visibility\n",
    "print(data_MV_train[0][2].shape) # classes\n",
    "print(len(data_MV_train[0][3])) # concept coordinates, but only the mask needs to be applied\n",
    "\n",
    "print(data_NoMV_train[0][0].shape) # image\n",
    "print(data_NoMV_train[0][1].shape) # concepts and visibility\n",
    "print(data_NoMV_train[0][2].shape) # classes\n",
    "print(len(data_NoMV_train[0][3])) # concept coordinates, but only the mask needs to be applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312 312\n"
     ]
    }
   ],
   "source": [
    "data_set = 'val' # majority voting is only done  \n",
    "\n",
    "#Define data set, the human transformer data_set is used to get the original images instead of the normalized ones\n",
    "transformer = get_inception_transform(mode=data_set,methode=\"center\")\n",
    "human_tansform = torchvision.transforms.Compose([torchvision.transforms.CenterCrop(299),torchvision.transforms.ToTensor()])\n",
    "\n",
    "#Get dataset\n",
    "data_MV_val = CUB_extnded_dataset(mode=data_set,config_dict=data_config_MV,transform=transformer, reduce = False)\n",
    "data_NoMV_val = CUB_extnded_dataset(mode=data_set,config_dict=data_config_NoMV,transform=transformer, reduce = False)\n",
    "\n",
    "#Get a dataset that return original images instead normalized ones\n",
    "data_human_MV_val = CUB_extnded_dataset(mode=data_set,config_dict=data_config_MV,transform=human_tansform, reduce = False)\n",
    "concept_names_MV_val = data_MV_val.consept_labels_names\n",
    "class_names_MV_val = data_MV_val.class_labels_names\n",
    "n_classes_MV_val = data_MV_val.n_classes\n",
    "n_concepts_MV_val = data_MV_val.n_concepts\n",
    "\n",
    "data_human_NoMV_val = CUB_extnded_dataset(mode=data_set,config_dict=data_config_NoMV,transform=human_tansform, reduce = False)\n",
    "concept_names_NoMV_val = data_NoMV_val.consept_labels_names\n",
    "class_names_NoMV_val = data_NoMV_val.class_labels_names\n",
    "n_classes_NoMV_val = data_NoMV_val.n_classes\n",
    "n_concepts_NoMV_val = data_NoMV_val.n_concepts\n",
    "\n",
    "\n",
    "print(n_concepts_MV_val,n_concepts_NoMV_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 299, 299])\n",
      "torch.Size([312])\n",
      "torch.Size([200])\n",
      "312\n",
      "torch.Size([3, 299, 299])\n",
      "torch.Size([2, 312])\n",
      "torch.Size([200])\n",
      "312\n"
     ]
    }
   ],
   "source": [
    "print(data_MV_val[0][0].shape) # image\n",
    "print(data_MV_val[0][1].shape) # concepts\n",
    "print(data_MV_val[0][2].shape) # classes\n",
    "print(len(data_MV_val[0][3])) # concept coordinates, but only the mask needs to be applied\n",
    "\n",
    "print(data_NoMV_val[0][0].shape) # image\n",
    "print(data_NoMV_val[0][1].shape) # concepts\n",
    "print(data_NoMV_val[0][2].shape) # classes\n",
    "print(len(data_NoMV_val[0][3])) # concept coordinates, but only the mask needs to be applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312 312\n"
     ]
    }
   ],
   "source": [
    "data_set = 'train' # majority voting is only done  \n",
    "\n",
    "#Define data set, the human transformer data_set is used to get the original images instead of the normalized ones\n",
    "transformer = get_inception_transform(mode=data_set,methode=\"center\")\n",
    "human_tansform = torchvision.transforms.Compose([torchvision.transforms.CenterCrop(299),torchvision.transforms.ToTensor()])\n",
    "\n",
    "#Get dataset\n",
    "data_MV_train = CUB_extnded_dataset(mode=data_set,config_dict=data_config_MV,transform=transformer, reduce = False)\n",
    "data_NoMV_train = CUB_extnded_dataset(mode=data_set,config_dict=data_config_NoMV,transform=transformer, reduce = False)\n",
    "\n",
    "#Get a dataset that return original images instead normalized ones\n",
    "data_human_MV_train = CUB_extnded_dataset(mode=data_set,config_dict=data_config_MV,transform=human_tansform, reduce = False)\n",
    "concept_names_MV_train = data_MV_train.consept_labels_names\n",
    "class_names_MV_train = data_MV_train.class_labels_names\n",
    "n_classes_MV_train = data_MV_train.n_classes\n",
    "n_concepts_MV_train = data_MV_train.n_concepts\n",
    "\n",
    "data_human_NoMV_train = CUB_extnded_dataset(mode=data_set,config_dict=data_config_NoMV,transform=human_tansform, reduce = False)\n",
    "concept_names_NoMV_train = data_NoMV_train.consept_labels_names\n",
    "class_names_NoMV_train = data_NoMV_train.class_labels_names\n",
    "n_classes_NoMV_train = data_NoMV_train.n_classes\n",
    "n_concepts_NoMV_train = data_NoMV_train.n_concepts\n",
    "\n",
    "\n",
    "print(n_concepts_MV_train,n_concepts_NoMV_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 299, 299])\n",
      "torch.Size([312])\n",
      "torch.Size([200])\n",
      "312\n",
      "torch.Size([3, 299, 299])\n",
      "torch.Size([2, 312])\n",
      "torch.Size([200])\n",
      "312\n"
     ]
    }
   ],
   "source": [
    "print(data_MV_train[0][0].shape) # image\n",
    "print(data_MV_train[0][1].shape) # concepts\n",
    "print(data_MV_train[0][2].shape) # classes\n",
    "print(len(data_MV_train[0][3])) # concept coordinates, but only the mask needs to be applied\n",
    "\n",
    "print(data_NoMV_train[0][0].shape) # image\n",
    "print(data_NoMV_train[0][1].shape) # concepts\n",
    "print(data_NoMV_train[0][2].shape) # classes\n",
    "print(len(data_NoMV_train[0][3])) # concept coordinates, but only the mask needs to be applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1198/1198 [00:40<00:00, 29.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(373776)\n",
      "373776\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "# compare all the concepts in data_MV_train to data_noMV_train\n",
    "same = 0\n",
    "for item_MV, item_NoMV in zip(tqdm(data_MV_val), data_NoMV_val):\n",
    "\n",
    "    same_i = torch.sum(item_MV[1] == item_NoMV[1][0])\n",
    "    same += same_i\n",
    "\n",
    "\n",
    "print(same)\n",
    "print(len(data_MV_val)*312)\n",
    "print(same/(len(data_MV_val)*312))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our data loader using the non reduced data, the MV and NoMV validation sets are exactly the same!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4796/4796 [02:29<00:00, 32.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1347208)\n",
      "1496352\n",
      "tensor(0.9003)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# compare all the concepts in data_MV_train to data_noMV_train\n",
    "same = 0\n",
    "for item_MV, item_NoMV in zip(tqdm(data_MV_train), data_NoMV_train):\n",
    "\n",
    "    same_i = torch.sum(item_MV[1] == item_NoMV[1][0])\n",
    "    same += same_i\n",
    "\n",
    "print(same)\n",
    "print(len(data_MV_train)*312)\n",
    "print(same/(len(data_MV_train)*312))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our dataloader using the non reduced data, 90% of the MV training set is the same as the NoMV training set. Thus we lost 10% of information.\n",
    "\n",
    "This could be noise, also it could just be variations in the bird species such as male and female ducks looking very different. Would be interesting to look into, but I don't know if we have time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312 312\n"
     ]
    }
   ],
   "source": [
    "data_set = 'val' # majority voting is only done  \n",
    "\n",
    "#Define data set, the human transformer data_set is used to get the original images instead of the normalized ones\n",
    "transformer = get_inception_transform(mode=data_set,methode=\"center\")\n",
    "human_tansform = torchvision.transforms.Compose([torchvision.transforms.CenterCrop(299),torchvision.transforms.ToTensor()])\n",
    "\n",
    "#Get dataset\n",
    "data_MV_val = CUB_extnded_dataset(mode=data_set,config_dict=data_config_MV,transform=transformer, reduce = False, andreas=True)\n",
    "data_NoMV_val = CUB_extnded_dataset(mode=data_set,config_dict=data_config_NoMV,transform=transformer, reduce = False, andreas=True)\n",
    "\n",
    "#Get a dataset that return original images instead normalized ones\n",
    "data_human_MV_val = CUB_extnded_dataset(mode=data_set,config_dict=data_config_MV,transform=human_tansform, reduce = False, andreas=True)\n",
    "concept_names_MV_val = data_MV_val.consept_labels_names\n",
    "class_names_MV_val = data_MV_val.class_labels_names\n",
    "n_classes_MV_val = data_MV_val.n_classes\n",
    "n_concepts_MV_val = data_MV_val.n_concepts\n",
    "\n",
    "data_human_NoMV_val = CUB_extnded_dataset(mode=data_set,config_dict=data_config_NoMV,transform=human_tansform, reduce = False, andreas=True)\n",
    "concept_names_NoMV_val = data_NoMV_val.consept_labels_names\n",
    "class_names_NoMV_val = data_NoMV_val.class_labels_names\n",
    "n_classes_NoMV_val = data_NoMV_val.n_classes\n",
    "n_concepts_NoMV_val = data_NoMV_val.n_concepts\n",
    "\n",
    "\n",
    "print(n_concepts_MV_val,n_concepts_NoMV_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 299, 299])\n",
      "torch.Size([312])\n",
      "torch.Size([200])\n",
      "312\n",
      "torch.Size([3, 299, 299])\n",
      "torch.Size([2, 312])\n",
      "torch.Size([200])\n",
      "312\n"
     ]
    }
   ],
   "source": [
    "print(data_MV_val[0][0].shape) # image\n",
    "print(data_MV_val[0][1].shape) # concepts\n",
    "print(data_MV_val[0][2].shape) # classes\n",
    "print(len(data_MV_val[0][3])) # concept coordinates, but only the mask needs to be applied\n",
    "\n",
    "print(data_NoMV_val[0][0].shape) # image\n",
    "print(data_NoMV_val[0][1].shape) # concepts\n",
    "print(data_NoMV_val[0][2].shape) # classes\n",
    "print(len(data_NoMV_val[0][3])) # concept coordinates, but only the mask needs to be applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312 312\n"
     ]
    }
   ],
   "source": [
    "data_set = 'train' # majority voting is only done  \n",
    "\n",
    "#Define data set, the human transformer data_set is used to get the original images instead of the normalized ones\n",
    "transformer = get_inception_transform(mode=data_set,methode=\"center\")\n",
    "human_tansform = torchvision.transforms.Compose([torchvision.transforms.CenterCrop(299),torchvision.transforms.ToTensor()])\n",
    "\n",
    "#Get dataset\n",
    "data_MV_train = CUB_extnded_dataset(mode=data_set,config_dict=data_config_MV,transform=transformer, reduce = False, andreas=True)\n",
    "data_NoMV_train = CUB_extnded_dataset(mode=data_set,config_dict=data_config_NoMV,transform=transformer, reduce = False, andreas=True)\n",
    "\n",
    "#Get a dataset that return original images instead normalized ones\n",
    "data_human_MV_train = CUB_extnded_dataset(mode=data_set,config_dict=data_config_MV,transform=human_tansform, reduce = False, andreas=True)\n",
    "concept_names_MV_train = data_MV_train.consept_labels_names\n",
    "class_names_MV_train = data_MV_train.class_labels_names\n",
    "n_classes_MV_train = data_MV_train.n_classes\n",
    "n_concepts_MV_train = data_MV_train.n_concepts\n",
    "\n",
    "data_human_NoMV_train = CUB_extnded_dataset(mode=data_set,config_dict=data_config_NoMV,transform=human_tansform, reduce = False, andreas=True)\n",
    "concept_names_NoMV_train = data_NoMV_train.consept_labels_names\n",
    "class_names_NoMV_train = data_NoMV_train.class_labels_names\n",
    "n_classes_NoMV_train = data_NoMV_train.n_classes\n",
    "n_concepts_NoMV_train = data_NoMV_train.n_concepts\n",
    "\n",
    "\n",
    "print(n_concepts_MV_train,n_concepts_NoMV_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 299, 299])\n",
      "torch.Size([312])\n",
      "torch.Size([200])\n",
      "312\n",
      "torch.Size([3, 299, 299])\n",
      "torch.Size([2, 312])\n",
      "torch.Size([200])\n",
      "312\n"
     ]
    }
   ],
   "source": [
    "print(data_MV_train[0][0].shape) # image\n",
    "print(data_MV_train[0][1].shape) # concepts\n",
    "print(data_MV_train[0][2].shape) # classes\n",
    "print(len(data_MV_train[0][3])) # concept coordinates, but only the mask needs to be applied\n",
    "\n",
    "print(data_NoMV_train[0][0].shape) # image\n",
    "print(data_NoMV_train[0][1].shape) # concepts\n",
    "print(data_NoMV_train[0][2].shape) # classes\n",
    "print(len(data_NoMV_train[0][3])) # concept coordinates, but only the mask needs to be applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1198/1198 [00:26<00:00, 44.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(336328)\n",
      "373776\n",
      "tensor(0.8998)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# compare all the concepts in data_MV_train to data_noMV_train\n",
    "same = 0\n",
    "for item_MV, item_NoMV in zip(tqdm(data_MV_val), data_NoMV_val):\n",
    "\n",
    "    same_i = torch.sum(item_MV[1] == item_NoMV[1][0])\n",
    "    same += same_i\n",
    "\n",
    "\n",
    "print(same)\n",
    "print(len(data_MV_val)*312)\n",
    "print(same/(len(data_MV_val)*312))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With Andreas' dataloader using the non reduced data, only 90% of the MV validation set is the same as the NoMV validation set. Thus we lost 10% of information. This is the same as for the training set as we expected and this does not make sense to do, since you are not suppose to know the true class of the validation set and thus you cannot perform MV on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4796/4796 [01:28<00:00, 54.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1347208)\n",
      "1496352\n",
      "tensor(0.9003)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# compare all the concepts in data_MV_train to data_noMV_train\n",
    "same = 0\n",
    "for item_MV, item_NoMV in zip(tqdm(data_MV_train), data_NoMV_train):\n",
    "\n",
    "    same_i = torch.sum(item_MV[1] == item_NoMV[1][0])\n",
    "    same += same_i\n",
    "\n",
    "print(same)\n",
    "print(len(data_MV_train)*312)\n",
    "print(same/(len(data_MV_train)*312))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again with Andreas' dataloader using the non reduced data, only 90% of the MV training set is the same as the NoMV training set. Thus we lost 10% of information. This is exactly the same af for our dataloader, which it is suppose to as it should not have changed the training set only the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112 312\n"
     ]
    }
   ],
   "source": [
    "data_set = 'val' # majority voting is only done  \n",
    "\n",
    "#Define data set, the human transformer data_set is used to get the original images instead of the normalized ones\n",
    "transformer = get_inception_transform(mode=data_set,methode=\"center\")\n",
    "human_tansform = torchvision.transforms.Compose([torchvision.transforms.CenterCrop(299),torchvision.transforms.ToTensor()])\n",
    "\n",
    "#Get dataset\n",
    "data_MV_val = CUB_extnded_dataset(mode=data_set,config_dict=data_config_MV,transform=transformer, andreas=True)\n",
    "data_NoMV_val = CUB_extnded_dataset(mode=data_set,config_dict=data_config_NoMV,transform=transformer, andreas=True)\n",
    "\n",
    "#Get a dataset that return original images instead normalized ones\n",
    "data_human_MV_val = CUB_extnded_dataset(mode=data_set,config_dict=data_config_MV,transform=human_tansform, andreas=True)\n",
    "concept_names_MV_val = data_MV_val.consept_labels_names\n",
    "class_names_MV_val = data_MV_val.class_labels_names\n",
    "n_classes_MV_val = data_MV_val.n_classes\n",
    "n_concepts_MV_val = data_MV_val.n_concepts\n",
    "\n",
    "data_human_NoMV_val = CUB_extnded_dataset(mode=data_set,config_dict=data_config_NoMV,transform=human_tansform, andreas=True)\n",
    "concept_names_NoMV_val = data_NoMV_val.consept_labels_names\n",
    "class_names_NoMV_val = data_NoMV_val.class_labels_names\n",
    "n_classes_NoMV_val = data_NoMV_val.n_classes\n",
    "n_concepts_NoMV_val = data_NoMV_val.n_concepts\n",
    "\n",
    "\n",
    "print(n_concepts_MV_val,n_concepts_NoMV_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 299, 299])\n",
      "torch.Size([112])\n",
      "torch.Size([200])\n",
      "112\n",
      "torch.Size([3, 299, 299])\n",
      "torch.Size([2, 312])\n",
      "torch.Size([200])\n",
      "312\n"
     ]
    }
   ],
   "source": [
    "print(data_MV_val[0][0].shape) # image\n",
    "print(data_MV_val[0][1].shape) # concepts\n",
    "print(data_MV_val[0][2].shape) # classes\n",
    "print(len(data_MV_val[0][3])) # concept coordinates, but only the mask needs to be applied\n",
    "\n",
    "print(data_NoMV_val[0][0].shape) # image\n",
    "print(data_NoMV_val[0][1].shape) # concepts\n",
    "print(data_NoMV_val[0][2].shape) # classes\n",
    "print(len(data_NoMV_val[0][3])) # concept coordinates, but only the mask needs to be applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112 312\n"
     ]
    }
   ],
   "source": [
    "data_set = 'train' # majority voting is only done  \n",
    "\n",
    "#Define data set, the human transformer data_set is used to get the original images instead of the normalized ones\n",
    "transformer = get_inception_transform(mode=data_set,methode=\"center\")\n",
    "human_tansform = torchvision.transforms.Compose([torchvision.transforms.CenterCrop(299),torchvision.transforms.ToTensor()])\n",
    "\n",
    "#Get dataset\n",
    "data_MV_train = CUB_extnded_dataset(mode=data_set,config_dict=data_config_MV,transform=transformer, andreas=True)\n",
    "data_NoMV_train = CUB_extnded_dataset(mode=data_set,config_dict=data_config_NoMV,transform=transformer, andreas=True)\n",
    "\n",
    "#Get a dataset that return original images instead normalized ones\n",
    "data_human_MV_train = CUB_extnded_dataset(mode=data_set,config_dict=data_config_MV,transform=human_tansform, andreas=True)\n",
    "concept_names_MV_train = data_MV_train.consept_labels_names\n",
    "class_names_MV_train = data_MV_train.class_labels_names\n",
    "n_classes_MV_train = data_MV_train.n_classes\n",
    "n_concepts_MV_train = data_MV_train.n_concepts\n",
    "\n",
    "data_human_NoMV_train = CUB_extnded_dataset(mode=data_set,config_dict=data_config_NoMV,transform=human_tansform, andreas=True)\n",
    "concept_names_NoMV_train = data_NoMV_train.consept_labels_names\n",
    "class_names_NoMV_train = data_NoMV_train.class_labels_names\n",
    "n_classes_NoMV_train = data_NoMV_train.n_classes\n",
    "n_concepts_NoMV_train = data_NoMV_train.n_concepts\n",
    "\n",
    "\n",
    "print(n_concepts_MV_train,n_concepts_NoMV_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 299, 299])\n",
      "torch.Size([112])\n",
      "torch.Size([200])\n",
      "112\n",
      "torch.Size([3, 299, 299])\n",
      "torch.Size([2, 312])\n",
      "torch.Size([200])\n",
      "312\n"
     ]
    }
   ],
   "source": [
    "print(data_MV_train[0][0].shape) # image\n",
    "print(data_MV_train[0][1].shape) # concepts\n",
    "print(data_MV_train[0][2].shape) # classes\n",
    "print(len(data_MV_train[0][3])) # concept coordinates, but only the mask needs to be applied\n",
    "\n",
    "print(data_NoMV_train[0][0].shape) # image\n",
    "print(data_NoMV_train[0][1].shape) # concepts\n",
    "print(data_NoMV_train[0][2].shape) # classes\n",
    "print(len(data_NoMV_train[0][3])) # concept coordinates, but only the mask needs to be applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112\n"
     ]
    }
   ],
   "source": [
    "data_set = 'val' # majority voting is only done  \n",
    "\n",
    "#Define data set, the human transformer data_set is used to get the original images instead of the normalized ones\n",
    "transformer = get_inception_transform(mode=data_set,methode=\"center\")\n",
    "human_tansform = torchvision.transforms.Compose([torchvision.transforms.CenterCrop(299),torchvision.transforms.ToTensor()])\n",
    "\n",
    "#Get dataset\n",
    "data_MV_val_andreas = CUB_extnded_dataset(mode=data_set,config_dict=data_config_MV,transform=transformer, andreas=True)\n",
    "\n",
    "concept_names_MV_val_andreas = data_MV_val_andreas.consept_labels_names\n",
    "class_names_MV_val_andreas = data_MV_val_andreas.class_labels_names\n",
    "n_classes_MV_val_andreas = data_MV_val_andreas.n_classes\n",
    "n_concepts_MV_val_andreas = data_MV_val_andreas.n_concepts\n",
    "\n",
    "print(n_concepts_MV_val_andreas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112\n"
     ]
    }
   ],
   "source": [
    "data_set = 'val' # majority voting is only done  \n",
    "\n",
    "#Define data set, the human transformer data_set is used to get the original images instead of the normalized ones\n",
    "transformer = get_inception_transform(mode=data_set,methode=\"center\")\n",
    "human_tansform = torchvision.transforms.Compose([torchvision.transforms.CenterCrop(299),torchvision.transforms.ToTensor()])\n",
    "\n",
    "#Get dataset\n",
    "data_MV_val = CUB_extnded_dataset(mode=data_set,config_dict=data_config_MV,transform=transformer)\n",
    "\n",
    "concept_names_MV_val = data_MV_val.consept_labels_names\n",
    "class_names_MV_val = data_MV_val.class_labels_names\n",
    "n_classes_MV_val = data_MV_val.n_classes\n",
    "n_concepts_MV_val = data_MV_val.n_concepts\n",
    "\n",
    "\n",
    "print(n_concepts_MV_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1198/1198 [00:41<00:00, 29.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(109327)\n",
      "373776\n",
      "tensor(0.2925)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# compare all the concepts in data_MV_train to data_noMV_train\n",
    "same = 0\n",
    "for item_MV, item_MV_andreas in zip(tqdm(data_MV_val), data_MV_val_andreas):\n",
    "\n",
    "    same_i = torch.sum(item_MV[1] == item_MV_andreas[1])\n",
    "    same += same_i\n",
    "\n",
    "print(same)\n",
    "print(len(data_MV_val)*312)\n",
    "print(same/(len(data_MV_val)*312))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Andreas' validation set and our validations set are very different! Now this is because we're now only looking at the reduced dataset, where we have looked at the non reduced dataset until now. The non reduced dataset contains a lot of zeros that will be the same for the no matter if MV is applied or not, thus when we are only looking at the reduced dataset the difference becomes more evident."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "con_bot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
